{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RUo7HHt8IO05"
      },
      "source": [
        "# Day 2 of AI Academy 2022 - NLP Track\n",
        "\n",
        "Hello! Welcome to the second day in your AI academy 2022, NLP track. In this lab we'll go over the usage of the HuggingFace's `transformers` library to train a BERT-based model for our sentiment analysis task. BERT is a transfromer-based model, a deep learning model, which requires more computation than the traditional models we worked with yesterday. Deep learning models typically require a GPU to train and run effciently, if you're local machine is not powered with a GPU to use, we recommend that you run this notebook in Google's Colabortatory (or Colab for short), which is a free notebook runtime that can assign you a GPU for a limited amount of time (usually 12 hours). If you wish to run this in Colab, simply click the following badge."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bzvKf53lIO08"
      },
      "source": [
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/Mostafa-Samir/AI-Academy-NLP-Dec-2022/blob/main/Day-2-lab.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4jyIfwTlIO08"
      },
      "source": [
        "## Colab Configuration\n",
        "\n",
        "If you chose to run in Colab, you'll need to run the following cell. \n",
        "\n",
        "_**DO NOT RUN THE NEXT CELL IF YOU'RE RUNNING THE NOTEBOOK LOCALLY!**_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2TTyiTFUIO09",
        "outputId": "9e416690-0f58-4f90-e32d-452748ee1af6"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/Mostafa-Samir/AI-Academy-NLP-Dec-2022.git\n",
        "\n",
        "!cd AI-Academy-NLP-Dec-2022 && \\\n",
        " pip install --upgrade pip && \\\n",
        " pip install -r requirements.txt && \\\n",
        " python -m setup-nltk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IvsPMMIhIO0-"
      },
      "source": [
        "## Data Path Resolution\n",
        "\n",
        "There'll be a slight difference in the file structure if you chose to run this notebook in Colab compared to running locally. The following script will take into account this difference in file structure to make sure sure that pathes to the data are correct in the rest of the notebook. The script will define a `DATA_ROOT` under which calling `DATA_ROOT/data/english/train.csv` for example will always be resolved correctly regardless of the environemnt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ttHEHGs1IO0-"
      },
      "outputs": [],
      "source": [
        "running_in_colab = 'google.colab' in str(get_ipython()) if hasattr(__builtins__,'__IPYTHON__') else False\n",
        "DATA_ROOT = \"./AI-Academy-NLP-Dec-2022\" if running_in_colab else \".\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GTXEF9f0IO0_"
      },
      "source": [
        "## Data Preparation\n",
        "\n",
        "To start using a BERT model to predict the sentiment of the text, we first need to prepare the data in the right format. Preperation here is much lighter than we did yesterday. Here, we'll be doing very light cleaning on the data by normalizing user mentions and possibly normalizing all URLs to a representative token. We'll then pass these lightly cleaned text into a pretrained unsupervised tokenizer like sentencepiece. We're chosing to do the light cleaning in order to show case the power of pertained unsupervised tockenizers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oypKAd1-IO0_"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def pipeline(fn_list):\n",
        "    def inner_function(text):\n",
        "        out = text\n",
        "        for fn in fn_list:\n",
        "            out = fn(out)\n",
        "        return out\n",
        "    \n",
        "    return inner_function\n",
        "\n",
        "def normalize_mentions(text: str) -> str:\n",
        "    return re.sub(\"@\\w*\", \"@user\", text)\n",
        "    \n",
        "def normalize_urls(text: str) -> str:\n",
        "    return re.sub(\"http(s{0,1})://[\\w\\-_./:]*\", \"http\", text)    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hku8ktK-IO1A"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "cleaning_pipeline = pipeline([normalize_mentions, normalize_urls])\n",
        "\n",
        "training_data = pd.read_csv(os.path.join(DATA_ROOT, \"data/english/train.csv\"))\n",
        "clean_training_data = training_data.copy()\n",
        "clean_training_data.loc[:, \"tweet\"] = clean_training_data.loc[:, \"tweet\"].apply(cleaning_pipeline)\n",
        "\n",
        "dev_data = pd.read_csv(os.path.join(DATA_ROOT, \"data/english/dev.csv\"))\n",
        "clean_dev_data = dev_data.copy()\n",
        "clean_dev_data.loc[:, \"tweet\"] = clean_dev_data.loc[:, \"tweet\"].apply(cleaning_pipeline)\n",
        "\n",
        "testing_data = pd.read_csv(os.path.join(DATA_ROOT, \"data/english/test.csv\"))\n",
        "clean_testing_data = testing_data.copy()\n",
        "clean_testing_data.loc[:, \"tweet\"] = clean_testing_data.loc[:, \"tweet\"].apply(cleaning_pipeline)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eW-yi3u-IO1A"
      },
      "source": [
        "### Tokenization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQVAhSsWIO1A"
      },
      "source": [
        "Now that we have our dataset lightly cleaned, we'll start looking at the tokenizer and how sentences are splitted into subword tokens for the BERT model. The pretrained BERT model that we'll be using is called `bert-base-cased`, of which we can get the tockenizer very easily using HuggingFace's `transformers` API. If the tokenizer object is available locally, the library will load it directly. Otherwise, it will be downloaded automatically from their hub and cached locally for later usage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5PZjcMzkIO1B"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xcbt-TvSIO1B"
      },
      "source": [
        "To get a glimpse of how subword tokenization work, let's take a comparitive look at a single tweet's tokens in two modes. The first mode is our regular split by space mode, and the other is by running the tweet through our pretrained tokenizer that we just instantiated."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4xT8hkK9IO1C",
        "outputId": "5caea70a-004c-4627-d188-2c3dcd610f1e"
      },
      "outputs": [],
      "source": [
        "tweet = clean_training_data.iloc[0 ,0]\n",
        "print(tweet.split(\" \"))\n",
        "print(\"\")\n",
        "print(tokenizer.tokenize(tweet))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IjEmKnv7IO1C"
      },
      "source": [
        "The first two lines in the output above is the the tokenization by whitespaces, the other two lines are for the pretrained tokenizer. We can see that the pretrained tokenizer is able to take a single token linke `tomorrow\\\\u002c` and tokenize it to multiple subwords `tomorrow, \\\\, u, ##00, ##2, ##c` hence recoverting the proper word tomorrow and splitting the other parts to tokens that it may have seen before. Another example is `Starkville`, where it could have not been seen in the training data, but the tokenizer have seen other samples postfixd with `ville` and may have learned that these represent locations, so it generates two subwords `Stark` and `##ville` to represent that single token. The double hashs we see in some of the tokens is an indicator of a subword. Sometimes the generated subword tokenization may not make direct sense to us humans, but the it makse statistical sense given the data that the tokenizer was pretrained on.\n",
        "\n",
        "_**Does it make some sense now that we only applied light cleaning?**_\n",
        "\n",
        "What we need to do now is start tokenizing all of our data into the format needed for training. This format has two main components to it:\n",
        "- The input_ids of the tokenized senetences. These are the numerical ids of the tokens in the pretrained vocabulary.\n",
        "- The attention mask, which represent what elements should be processed by the model and what shouldn't. This is important because we need to make all the representations have the same sequence length so that we can process them in a parallel and memory effcient way, and this could result in adding extra non-informative `PAD` symbols to the tokens. The attention mask will have 0 for these non-informative pad tokens, and 1 for the other informative original tokens of the sentence.\n",
        "\n",
        "Calling the tokenizer directly on the sentences will result in these two pieces of data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VerNsVi3IO1C"
      },
      "outputs": [],
      "source": [
        "tokenized_training_data = tokenizer(\n",
        "    clean_training_data.loc[:, \"tweet\"].to_list(),\n",
        "    padding='longest',\n",
        "    return_tensors='pt',\n",
        "    return_attention_mask = True,\n",
        "    return_token_type_ids=False\n",
        ")\n",
        "\n",
        "tokenized_dev_data = tokenizer(\n",
        "    clean_dev_data.loc[:, \"tweet\"].to_list(),\n",
        "    padding='longest',\n",
        "    return_tensors='pt',\n",
        "    return_attention_mask = True,\n",
        "    return_token_type_ids=False\n",
        ")\n",
        "\n",
        "tokenized_testing_data = tokenizer(\n",
        "    clean_testing_data.loc[:, \"tweet\"].to_list(),\n",
        "    padding='longest',\n",
        "    return_tensors='pt',\n",
        "    return_attention_mask = True,\n",
        "    return_token_type_ids=False\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U77IGh8FIO1C",
        "outputId": "0af5fc24-3d03-4dfb-f38d-f07a36f52cad"
      },
      "outputs": [],
      "source": [
        "tokenized_dev_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uenNOjEEIO1D"
      },
      "source": [
        "Before we can finalize the preparation of our dataset, we need first to encode our labels into numeric values, we'll set negative to be 0, neutral to be 1 and positive to be 2."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sIlZmz_9IO1D"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "label_to_id = {\"negative\": 0, \"neutral\": 1, \"positive\": 2}\n",
        "\n",
        "training_labels = torch.tensor([label_to_id[label] for label in clean_training_data.loc[:, \"label\"]])\n",
        "dev_labels = torch.tensor([label_to_id[label] for label in clean_dev_data.loc[:, \"label\"]])\n",
        "testing_labels = torch.tensor([label_to_id[label] for label in clean_testing_data.loc[:, \"label\"]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OHVqief0Kh9T",
        "outputId": "03345421-b494-4cce-be22-de10b5dc8c92"
      },
      "outputs": [],
      "source": [
        "dev_labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pfwfNBVdIO1D"
      },
      "source": [
        "Now that we have tokenized all of our data, we only need to wrap them into a `pytorch`'s `Dataset` class. This `Dataset` class is a convenient class used to facilitate generating batches for the training. What we'll do is to define a generic dataset class that is not tied to any of the specific splits, and then use that to instantiate different dataset objects for each of the splits."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dhQvTqkXIO1D"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "class SentimentDataset(Dataset):\n",
        "\n",
        "    def __init__(self, input_ids, attention_masks, encoded_labels):\n",
        "        self.input_ids = input_ids\n",
        "        self.attention_masks = attention_masks\n",
        "        self.encoded_labels = encoded_labels\n",
        "\n",
        "        self.size, *_ = input_ids.shape\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.size\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return {\n",
        "            \"input_ids\": self.input_ids[index],\n",
        "            \"attention_mask\": self.attention_masks[index],\n",
        "            \"labels\": self.encoded_labels[index]\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cbOyjXQ3IO1D"
      },
      "outputs": [],
      "source": [
        "training_dataset = SentimentDataset(tokenized_training_data[\"input_ids\"], tokenized_training_data[\"attention_mask\"], training_labels)\n",
        "dev_dataset = SentimentDataset(tokenized_dev_data[\"input_ids\"], tokenized_dev_data[\"attention_mask\"], dev_labels)\n",
        "testing_dataset = SentimentDataset(tokenized_testing_data[\"input_ids\"], tokenized_testing_data[\"attention_mask\"], testing_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whRo-5MJIO1E"
      },
      "source": [
        "## Training a BERT-based Classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TP2F_cjcIO1E"
      },
      "source": [
        "Now that we have our datset ready, we can start preparing our model for training. First we need to get an instance of the BERT pretrained weights to use in our model. This can simply be done via HuggingFace's APIs in a similar way for the tokenizer; by using the `AutoModelForSequenceClassification`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aC0hfhKAIO1E",
        "outputId": "a99f34db-e8cb-4753-f8fb-03ee859f0520"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-uncased\",\n",
        "    num_labels=3,\n",
        "    problem_type=\"single_label_classification\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lnv4ftTBIO1E"
      },
      "source": [
        "As we can see, the `transformers` package will download the pretrained weights from the hub and cache them locally like it did with the tokenizer earlier. As we discussed earlier in the first session, this BERT model is trained on the maksed langauge modeling task (MLM), so it doesn't work directly for text classification. What `AutoModelForSequenceClassification` does is that it takes the pretrained weights from the MLM task, and appends a feed forward neural network on top of its output. This feed forward NN has an output size equal to the number of labels in our data so that it will able to predict them correctly. This feedforward NN is intitialized randomly and not trained, but it will get trained along with the other weights of BERT when we present our training data to it. This process is what we call **fine-tuning**.\n",
        "\n",
        "To do that fine-tuning, we'll utilize the `Trainer` object from the `transformers` library. But in order to do so, we need first to provide two external objects, the first one is a metrics computation function. This function accepts an object called `evaluation_prediction` from the training loop and this object contains both the model's predictions and the true labels for a batch. We'll write a function that consumes this object and returns a dictionary of metrics, one of them should be used to track the model performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ka-6KDcIO1E"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import f1_score, recall_score\n",
        "\n",
        "def compute_metrics(evaluation_predictions):\n",
        "\n",
        "    logits = evaluation_predictions.predictions\n",
        "    y_true = evaluation_predictions.label_ids\n",
        "\n",
        "    y_pred = np.argmax(logits, axis=1)\n",
        "    \n",
        "\n",
        "    weighted_f1 = f1_score(y_true, y_pred, average='weighted')\n",
        "    negative_f1, *_ = f1_score(y_true, y_pred, labels=[0], average=None)\n",
        "    neutral_f1, *_ = f1_score(y_true, y_pred, labels=[1], average=None)\n",
        "    positive_f1, *_ = f1_score(y_true, y_pred, labels=[2], average=None)\n",
        "\n",
        "    negative_recall, *_ = recall_score(y_true, y_pred, labels=[0], average=None)\n",
        "\n",
        "    return {\n",
        "        \"weighted-F1\": weighted_f1,\n",
        "        \"negative-recall\": negative_recall,\n",
        "        \"negative-F1\": negative_f1,\n",
        "        \"neutral-F1\": neutral_f1,\n",
        "        \"positive_f1\": positive_f1\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30jhkW7RIO1E"
      },
      "source": [
        "After defining the `compute_metrics` function, we'll go on a define the next object required to run the `Traininer`, which is the training arguments. You can think of this as a bunch of configurations that determine how the training should run. This includes the training batch size, how many epochs to train for, when to save for checkpoints, how many checkpoints we should save on disk, what is the metric (out of the two we have defined above) is the one to monitor ... etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NLd1-23qIO1F",
        "outputId": "c48ce968-83fd-4b3e-9888-2ca023e702d5"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "batch_size = 16\n",
        "metric_name = \"negative-recall\"\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy = \"epoch\",\n",
        "    learning_rate=1e-6,\n",
        "    num_train_epochs=50,\n",
        "    load_best_model_at_end=True,\n",
        "    save_total_limit=2,\n",
        "    metric_for_best_model=metric_name,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    output_dir=\"./models\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KNi6BAavIO1F"
      },
      "source": [
        "We're ready now to train our model using all the pieces that we have been defining so far. We just need to import `Trainer` from transformers and intialize a trainer object with our specifications."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y7fFzDlQIO1F",
        "outputId": "514380c8-e096-4a31-b8d4-17e92cbe3e85"
      },
      "outputs": [],
      "source": [
        "from transformers import Trainer\n",
        "\n",
        "trainier = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=training_dataset,\n",
        "    eval_dataset=dev_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39eG7ErqIO1F"
      },
      "source": [
        "We simply run the training now by calling `trainer.train()`!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "BG8XvdusIO1F",
        "outputId": "175737f4-d7d3-4fa9-91e9-57b7b33db341"
      },
      "outputs": [],
      "source": [
        "trainier.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2OvAHMurIO1F"
      },
      "source": [
        "Now that we have our best trained model, we can evaluate it on the test set using the `trainer.predict` method which returns the same named tuple object that we can use with the `compute_metrics` method we have defined"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_predictions = trainier.predict(test_dataset=testing_dataset)\n",
        "compute_metrics(test_predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, we save our model for the next lab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "save_dir = \"./bert_model\"\n",
        "\n",
        "tokenizer.save_pretrained(save_dir)\n",
        "model.save_pretrained(save_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!zip -r bert_model.zip ./bert_model"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.8.15 ('ai-academy-env')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.15"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "639172ddd52cbf2c82750a1f8e24033e05c7be0e91547485949b41530ebe4e23"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
